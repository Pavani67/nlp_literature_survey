{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9fbb22f4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-24T05:48:08.998075Z",
          "iopub.status.busy": "2022-09-24T05:48:08.997631Z",
          "iopub.status.idle": "2022-09-24T05:48:09.032632Z",
          "shell.execute_reply": "2022-09-24T05:48:09.031251Z",
          "shell.execute_reply.started": "2022-09-24T05:48:08.997977Z"
        },
        "papermill": {
          "duration": 0.012577,
          "end_time": "2022-10-08T06:12:27.582540",
          "exception": false,
          "start_time": "2022-10-08T06:12:27.569963",
          "status": "completed"
        },
        "tags": [],
        "id": "9fbb22f4"
      },
      "source": [
        "\n",
        "<h1 align=\"center\" style=\"color:green;font-size: 3em;\" >Building a Question Answering System Using Bert</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc0738e",
      "metadata": {
        "papermill": {
          "duration": 0.010595,
          "end_time": "2022-10-08T06:12:27.775139",
          "exception": false,
          "start_time": "2022-10-08T06:12:27.764544",
          "status": "completed"
        },
        "tags": [],
        "id": "afc0738e"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<a class=\"anchor\" id=\"section2\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">Simple Inference Pipeline on Pretrained Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce939473",
      "metadata": {
        "papermill": {
          "duration": 0.010606,
          "end_time": "2022-10-08T06:12:27.796542",
          "exception": false,
          "start_time": "2022-10-08T06:12:27.785936",
          "status": "completed"
        },
        "tags": [],
        "id": "ce939473"
      },
      "source": [
        "Before processing further Let me make a sample inference and show how the input and prediction should looks like. Lets load the tokenizer and model first"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJhokVWCRXmV",
        "outputId": "8bdce5ee-0f20-4f79-ecdc-7eba0ae654e5"
      },
      "id": "gJhokVWCRXmV",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bbac6722",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:12:27.820079Z",
          "iopub.status.busy": "2022-10-08T06:12:27.819568Z",
          "iopub.status.idle": "2022-10-08T06:12:47.030346Z",
          "shell.execute_reply": "2022-10-08T06:12:47.029219Z"
        },
        "papermill": {
          "duration": 19.225841,
          "end_time": "2022-10-08T06:12:47.033191",
          "exception": false,
          "start_time": "2022-10-08T06:12:27.807350",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "7ed9d7a09e344f7c885291fa735718c1",
            "065c29204bc7485394160692561ec1fc",
            "1c45b7c90a144f28b0a32998a5279f8e",
            "56b676da20c24ec78f4860cac60a4c3f",
            "3aedd62bc93242658f99590834dc48df",
            "b6858a4ee3f64071aa506bc8c2ef0a71",
            "0a4c9647a8ef4b90bdcc34cd7595ba18",
            "7b618e5eba284d568afa4f7290f97b8a",
            "788bc22ebad149c899718836a60cdf72",
            "0b73557f8563429889ea87cbbe3a58dd",
            "621ae6f6daed4fa691409524d78cad99",
            "a0985202a2304e36b01a3eb5be557151",
            "d37df4a29ce64a08ad027e4b92015fd0",
            "c05dfa5df68849f4be9d020955a82537",
            "a5f3aba34aee46888ce139b458438827",
            "71429803b40e42c79c6f6a3b801321f8",
            "aec1cfab5dc44211b3baf15d257c9cc1",
            "2c35dd6724b74cd285ce7d312f5c0d20",
            "1ef02c3cacd7410bafdd94897cfa0837",
            "8987e161b7ba445ca7eec571d7630007",
            "45c9b2cc4c2747e1b443de3ab99a9bd4",
            "506b9c508a6f44f9832bfd66f7ef34fa",
            "8ae938b92d274f658f9f64659402839d",
            "d87a0ed68252402d992f3156e4dd2a96",
            "4fc50db278024a6b8577d39f07631718",
            "571b746f58c542e7bb1bcdfe18fd9787",
            "c3fb4aac484645e4be878cfbf1d7f00f",
            "716c4a00b1c24dbaa47cefec6b7432b2",
            "235f3f1d7e2e44d2bcfdc4c385e857da",
            "6c72c18c7b6345b8b448482418bfe1eb",
            "d746f2a4c4aa4cb28e436e69004d8933",
            "1e142a48568d405ebca7daaec4d061a3",
            "78d42edd8719425096cb5b814f65cf0a",
            "39464cbb371f44ae8991adc23ed2f847",
            "784929e9e4fe43cdbe37b54bce84bf23",
            "f8a016c165004ed7888709bfe66b45de",
            "c0afb74e5b514a03bae9fd83109f9e75",
            "76e2a243cebb40cbadccbdce944d492f",
            "cdcc5e74b6bc4ce893fee4cf872f96c1",
            "4779e6adfb214f218a97c4257d3e4eb4",
            "07ad6a25576f4a1bb41a0bfb12b3c8f9",
            "8bd7b24824ff483dbdb1e9096b7b949c",
            "f30b27f883dd4d0a908993d317410ee0",
            "5c17c34a2c0c45b0bb50ab15f7116121",
            "88dfa753c47347d9881050beaa8095e4",
            "7dfe37c2201642ce8dafa65a70ae543b",
            "f5bec9b1ace1458b995cc34e73ac92a5",
            "d7b2805f0b2642de8e7fbb5f1bb84737",
            "516774e959eb4ba9b6306886d8c3c078",
            "a1bfc5e73f8c4707a5f2ea05aadd6093",
            "ea4f818cfb2d46eb9e19edc75483f1b4",
            "75a5b3c0aab74568ad0152c5229a9764",
            "cc076d31ed534296a2f2389fce261fc1",
            "543be14283d847288d2929e16bf08353",
            "f996e84eb3a54ef9b229fc4aa58fa8bf"
          ]
        },
        "id": "bbac6722",
        "outputId": "637e099f-541b-48a8-97fd-0fae1c447479"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ed9d7a09e344f7c885291fa735718c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0985202a2304e36b01a3eb5be557151"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ae938b92d274f658f9f64659402839d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39464cbb371f44ae8991adc23ed2f847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88dfa753c47347d9881050beaa8095e4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "weight_path = \"kaporter/bert-base-uncased-finetuned-squad\"\n",
        "# loading tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(weight_path)\n",
        "#loading the model\n",
        "model = BertForQuestionAnswering.from_pretrained(weight_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2649adab",
      "metadata": {
        "papermill": {
          "duration": 0.01134,
          "end_time": "2022-10-08T06:12:47.056930",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.045590",
          "status": "completed"
        },
        "tags": [],
        "id": "2649adab"
      },
      "source": [
        "Lets take an example\n",
        "\n",
        "```\n",
        "question = \"How many parameters does BERT-large have?\"\n",
        "\n",
        "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
        "\n",
        "answer = 340M\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561dc3bd",
      "metadata": {
        "papermill": {
          "duration": 0.011245,
          "end_time": "2022-10-08T06:12:47.079609",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.068364",
          "status": "completed"
        },
        "tags": [],
        "id": "561dc3bd"
      },
      "source": [
        "Now lets generate token_ids using tokenizer and see it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f7027e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:12:47.104876Z",
          "iopub.status.busy": "2022-10-08T06:12:47.103691Z",
          "iopub.status.idle": "2022-10-08T06:12:47.116953Z",
          "shell.execute_reply": "2022-10-08T06:12:47.114958Z"
        },
        "papermill": {
          "duration": 0.027913,
          "end_time": "2022-10-08T06:12:47.118899",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.090986",
          "status": "completed"
        },
        "tags": [],
        "id": "12f7027e",
        "outputId": "cb8fc179-2348-47aa-b74d-9cad1173e934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have about 70 tokens generated\n",
            " \n",
            "Some examples of token-input_id pairs:\n",
            "[CLS] : 101\n",
            "how : 2129\n",
            "many : 2116\n",
            "parameters : 11709\n",
            "does : 2515\n",
            "bert : 14324\n",
            "- : 1011\n",
            "large : 2312\n",
            "have : 2031\n",
            "? : 1029\n",
            "[SEP] : 102\n",
            "bert : 14324\n",
            "- : 1011\n",
            "large : 2312\n",
            "is : 2003\n",
            "really : 2428\n",
            "big : 2502\n",
            ". : 1012\n",
            ". : 1012\n",
            ". : 1012\n",
            "it : 2009\n",
            "has : 2038\n",
            "24 : 2484\n",
            "- : 1011\n",
            "layers : 9014\n",
            "and : 1998\n",
            "an : 2019\n",
            "em : 7861\n",
            "##bed : 8270\n",
            "##ding : 4667\n",
            "size : 2946\n",
            "of : 1997\n",
            "1 : 1015\n",
            ", : 1010\n",
            "02 : 6185\n",
            "##4 : 2549\n",
            ", : 1010\n",
            "for : 2005\n",
            "a : 1037\n",
            "total : 2561\n",
            "of : 1997\n",
            "340 : 16029\n",
            "##m : 2213\n",
            "parameters : 11709\n",
            "! : 999\n",
            "altogether : 10462\n",
            "it : 2009\n",
            "is : 2003\n",
            "1 : 1015\n",
            ". : 1012\n",
            "34 : 4090\n",
            "##gb : 18259\n",
            ", : 1010\n",
            "so : 2061\n",
            "expect : 5987\n",
            "it : 2009\n",
            "to : 2000\n",
            "take : 2202\n",
            "a : 1037\n",
            "couple : 3232\n",
            "minutes : 2781\n",
            "to : 2000\n",
            "download : 8816\n",
            "to : 2000\n",
            "your : 2115\n",
            "cola : 15270\n",
            "##b : 2497\n",
            "instance : 6013\n",
            ". : 1012\n",
            "[SEP] : 102\n"
          ]
        }
      ],
      "source": [
        "question = \"How many parameters does BERT-large have?\"\n",
        "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
        "\n",
        "input_ids = tokenizer.encode(question, context)\n",
        "print (f'We have about {len(input_ids)} tokens generated')\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(\" \")\n",
        "print('Some examples of token-input_id pairs:')\n",
        "\n",
        "for i, (token,inp_id) in enumerate(zip(tokens,input_ids)):\n",
        "    print(token,\":\",inp_id)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68fd9ff6",
      "metadata": {
        "papermill": {
          "duration": 0.011358,
          "end_time": "2022-10-08T06:12:47.141836",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.130478",
          "status": "completed"
        },
        "tags": [],
        "id": "68fd9ff6"
      },
      "source": [
        "\n",
        "Lets generate segmentation embedding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2108d1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:12:47.166553Z",
          "iopub.status.busy": "2022-10-08T06:12:47.165731Z",
          "iopub.status.idle": "2022-10-08T06:12:47.172041Z",
          "shell.execute_reply": "2022-10-08T06:12:47.170735Z"
        },
        "papermill": {
          "duration": 0.020612,
          "end_time": "2022-10-08T06:12:47.173929",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.153317",
          "status": "completed"
        },
        "tags": [],
        "id": "5d2108d1",
        "outputId": "54604189-0121-4268-9464-9b4674961625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "sep_idx = tokens.index('[SEP]')\n",
        "\n",
        "# we will provide including [SEP] token which seperates question from context and 1 for rest.\n",
        "token_type_ids = [0 for i in range(sep_idx+1)] + [1 for i in range(sep_idx+1,len(tokens))]\n",
        "print(token_type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fae7543",
      "metadata": {
        "papermill": {
          "duration": 0.011329,
          "end_time": "2022-10-08T06:12:47.196857",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.185528",
          "status": "completed"
        },
        "tags": [],
        "id": "1fae7543"
      },
      "source": [
        "Now lets pass our input through model and sees the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea4f9835",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:12:47.221472Z",
          "iopub.status.busy": "2022-10-08T06:12:47.220674Z",
          "iopub.status.idle": "2022-10-08T06:12:47.551682Z",
          "shell.execute_reply": "2022-10-08T06:12:47.550560Z"
        },
        "papermill": {
          "duration": 0.346272,
          "end_time": "2022-10-08T06:12:47.554623",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.208351",
          "status": "completed"
        },
        "tags": [],
        "id": "ea4f9835",
        "outputId": "f565e166-bf28-4a49-c2c9-3793924f1698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted answer: 340\n"
          ]
        }
      ],
      "source": [
        "# Run our example through the model.\n",
        "out = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                token_type_ids=torch.tensor([token_type_ids]))\n",
        "\n",
        "start_logits,end_logits = out['start_logits'],out['end_logits']\n",
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "answer_start = torch.argmax(start_logits)\n",
        "answer_end = torch.argmax(end_logits)\n",
        "\n",
        "ans = ''.join(tokens[answer_start:answer_end])\n",
        "print('Predicted answer:', ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda00f86",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:12:47.579717Z",
          "iopub.status.busy": "2022-10-08T06:12:47.579389Z",
          "iopub.status.idle": "2022-10-08T06:12:47.584892Z",
          "shell.execute_reply": "2022-10-08T06:12:47.583897Z"
        },
        "papermill": {
          "duration": 0.020345,
          "end_time": "2022-10-08T06:12:47.586878",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.566533",
          "status": "completed"
        },
        "tags": [],
        "id": "cda00f86"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "del tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dccf2f88",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T08:37:00.339610Z",
          "iopub.status.busy": "2022-09-28T08:37:00.338226Z",
          "iopub.status.idle": "2022-09-28T08:37:00.347174Z",
          "shell.execute_reply": "2022-09-28T08:37:00.345188Z",
          "shell.execute_reply.started": "2022-09-28T08:37:00.339546Z"
        },
        "papermill": {
          "duration": 0.011343,
          "end_time": "2022-10-08T06:12:47.609815",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.598472",
          "status": "completed"
        },
        "tags": [],
        "id": "dccf2f88"
      },
      "source": [
        "We have seen that how we can predict using a finetuned bert(bert-base) model. Now lets train and model on Squad dataest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8f7e42",
      "metadata": {
        "papermill": {
          "duration": 0.013761,
          "end_time": "2022-10-08T06:12:47.637590",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.623829",
          "status": "completed"
        },
        "tags": [],
        "id": "db8f7e42"
      },
      "source": [
        "\n",
        "<a class=\"anchor\" id=\"section3\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">Understanding Data Preprocessing required for Question-Answering System</h2>\n",
        "\n",
        "In this section before getting to the training part, let us understand how we process train data and validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "194a8e9c",
      "metadata": {
        "papermill": {
          "duration": 0.012812,
          "end_time": "2022-10-08T06:12:47.663495",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.650683",
          "status": "completed"
        },
        "tags": [],
        "id": "194a8e9c"
      },
      "source": [
        "```\n",
        "Note: For training and demo we will use distil bert instead of bert as it has less parameters and so consume less memory.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733a2818",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:12:47.689628Z",
          "iopub.status.busy": "2022-10-08T06:12:47.688625Z",
          "iopub.status.idle": "2022-10-08T06:12:48.549938Z",
          "shell.execute_reply": "2022-10-08T06:12:48.548965Z"
        },
        "papermill": {
          "duration": 0.87713,
          "end_time": "2022-10-08T06:12:48.552795",
          "exception": false,
          "start_time": "2022-10-08T06:12:47.675665",
          "status": "completed"
        },
        "tags": [],
        "id": "733a2818"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b82ed5e",
      "metadata": {
        "papermill": {
          "duration": 0.011576,
          "end_time": "2022-10-08T06:12:48.576330",
          "exception": false,
          "start_time": "2022-10-08T06:12:48.564754",
          "status": "completed"
        },
        "tags": [],
        "id": "1b82ed5e"
      },
      "source": [
        "**About dataset**\n",
        "\n",
        "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "409145d3",
      "metadata": {
        "papermill": {
          "duration": 0.011394,
          "end_time": "2022-10-08T06:12:48.599324",
          "exception": false,
          "start_time": "2022-10-08T06:12:48.587930",
          "status": "completed"
        },
        "tags": [],
        "id": "409145d3"
      },
      "source": [
        "**Loading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd813c6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:12:48.624916Z",
          "iopub.status.busy": "2022-10-08T06:12:48.623426Z",
          "iopub.status.idle": "2022-10-08T06:13:01.150195Z",
          "shell.execute_reply": "2022-10-08T06:13:01.149142Z"
        },
        "papermill": {
          "duration": 12.541406,
          "end_time": "2022-10-08T06:13:01.152331",
          "exception": false,
          "start_time": "2022-10-08T06:12:48.610925",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "39fd483899914f54b85cd0b1fa6d1366",
            "44944407f230400686f2d65f6f466467",
            "4ed1732ea9db4fcfa69a2bf2f6017c12",
            "7849f9499eed44579bf9436a0a5ed65e",
            "a4fa26e028c34135b90960f52408e5ba",
            "1d0ba1142d9b444baae9aa9510d97aa2",
            "8d9489b4e27d4f85bb1f377f92e81cad",
            "5ab5ff063db34960a7bcc73771332f25",
            "d1eb521f182a4a98a2de3f0b7e94b409"
          ]
        },
        "id": "dbd813c6",
        "outputId": "6144de72-f3b7-427e-ecc4-a600e13b32ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39fd483899914f54b85cd0b1fa6d1366",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44944407f230400686f2d65f6f466467",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ed1732ea9db4fcfa69a2bf2f6017c12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7849f9499eed44579bf9436a0a5ed65e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4fa26e028c34135b90960f52408e5ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d0ba1142d9b444baae9aa9510d97aa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d9489b4e27d4f85bb1f377f92e81cad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ab5ff063db34960a7bcc73771332f25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1eb521f182a4a98a2de3f0b7e94b409",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea712336",
      "metadata": {
        "papermill": {
          "duration": 0.012996,
          "end_time": "2022-10-08T06:13:01.178994",
          "exception": false,
          "start_time": "2022-10-08T06:13:01.165998",
          "status": "completed"
        },
        "tags": [],
        "id": "ea712336"
      },
      "source": [
        "We have about 87599 data points in train and 10570 in validation. \n",
        "Lets see some sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36f4727",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:01.206726Z",
          "iopub.status.busy": "2022-10-08T06:13:01.206441Z",
          "iopub.status.idle": "2022-10-08T06:13:01.215852Z",
          "shell.execute_reply": "2022-10-08T06:13:01.214931Z"
        },
        "papermill": {
          "duration": 0.026314,
          "end_time": "2022-10-08T06:13:01.218387",
          "exception": false,
          "start_time": "2022-10-08T06:13:01.192073",
          "status": "completed"
        },
        "tags": [],
        "id": "a36f4727",
        "outputId": "e9ed4293-7abb-4f8b-a86e-50ed0e7ea4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mTrain Data Sample.....\u001b[0;0m\n",
            " \n",
            "\u001b[1mID -\u001b[0;0m 5733be284776f41900661182\n",
            "\u001b[1mTITLE - \u001b[0;0m University_of_Notre_Dame\n",
            "\u001b[1mCONTEXT - \u001b[0;0m Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "\u001b[1mANSWERS - \u001b[0;0m ['Saint Bernadette Soubirous']\n",
            "\u001b[1mANSWERS START INDEX - \u001b[0;0m [515]\n",
            " \n",
            "------------------------------------------------------------------------------------------\n",
            "\u001b[1mValidation Data Sample.....\u001b[0;0m\n",
            " \n",
            "\u001b[1mID -\u001b[0;0m 56be4db0acb8001400a502ec\n",
            "\u001b[1mTITLE - \u001b[0;0m Super_Bowl_50\n",
            "\u001b[1mCONTEXT - \u001b[0;0m Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "\u001b[1mANSWERS - \u001b[0;0m ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
            "\u001b[1mANSWERS START INDEX - \u001b[0;0m [177, 177, 177]\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# to make text bold\n",
        "s_bold = '\\033[1m'\n",
        "e_bold = '\\033[0;0m'\n",
        "\n",
        "print(s_bold + 'Train Data Sample.....' + e_bold)\n",
        "train_data = dataset[\"train\"]\n",
        "for data in train_data:\n",
        "    print(' ')\n",
        "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
        "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
        "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
        "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
        "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
        "    print(' ')\n",
        "    break\n",
        "    \n",
        "print('---'*30)   \n",
        "print(s_bold + 'Validation Data Sample.....' + e_bold)\n",
        "train_data = dataset[\"validation\"]\n",
        "for data in train_data:\n",
        "    print(' ')\n",
        "    print(s_bold + 'ID -' + e_bold, data['id'])\n",
        "    print(s_bold +'TITLE - '+ e_bold, data['title'])\n",
        "    print(s_bold + 'CONTEXT - '+ e_bold,data['context'])\n",
        "    print(s_bold + 'ANSWERS - ' + e_bold,data['answers']['text'])\n",
        "    print(s_bold + 'ANSWERS START INDEX - ' + e_bold,data['answers']['answer_start'])\n",
        "    print(' ')\n",
        "    break\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d54748",
      "metadata": {
        "papermill": {
          "duration": 0.012929,
          "end_time": "2022-10-08T06:13:01.244417",
          "exception": false,
          "start_time": "2022-10-08T06:13:01.231488",
          "status": "completed"
        },
        "tags": [],
        "id": "81d54748"
      },
      "source": [
        "We can see a title, Context and Answers along with stat indexes.We might need a little preprocessing of text before feeding to Bert Model. As discussed in our earlier article, input to our Bert Model is a sum of token embeddings, positional embeddings and segmentation embedddings. Lets see how out input looks like for a question answering system.\n",
        "\n",
        "Lets take an example:\n",
        "\n",
        "```\n",
        "Question: \"Which is your favorite sport?\"\n",
        "\n",
        "Reference text: \"Iam Jhon.My favorite sport is football.I happily live in Florida. \n",
        "\n",
        "Answer: football.\n",
        "\n",
        "````\n",
        "\n",
        "After tokenization our input looks like this - \n",
        "\n",
        "\n",
        "```\n",
        "[[CLS],\"Which\",\"is\",\"your\",\"favorite\",\"sport\",\"?\",[SEP],\"My\",\"favourite\",\"sport\",\"is\",\"football\",\".\",\"I\",\"happy\",\"###ly\",\"live\",\"in\",\"Florida\",[SEP]]\n",
        "\n",
        "```\n",
        "\n",
        "Thus it will be in format  [CLS] question [SEP] context [SEP]\n",
        "\n",
        "\n",
        "\n",
        "As next step suitable padding is added and is converted to word ids. Finally it is mapped with embedding matrx to generate Embedding vector. Similary we will have a corresponsing positional embedding vector.\n",
        "\n",
        "Next we will see how our segmentation embedding looks like\n",
        "```\n",
        "\n",
        "[0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "\n",
        "```\n",
        "\n",
        "It will be 0's for tokens corresponding to first sentence and 1 for tokens corresponding to second setance. It is mainly used to distinguish between 2 inputs.\n",
        "Finally these three are summed and will becomes input to our Bert Model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96fecea",
      "metadata": {
        "papermill": {
          "duration": 0.012927,
          "end_time": "2022-10-08T06:13:01.270433",
          "exception": false,
          "start_time": "2022-10-08T06:13:01.257506",
          "status": "completed"
        },
        "tags": [],
        "id": "b96fecea"
      },
      "source": [
        "Another important thing that we found is that we can see multiple answers in one of our validation sample.Lets analyze it further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf2c466",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:01.298369Z",
          "iopub.status.busy": "2022-10-08T06:13:01.298090Z",
          "iopub.status.idle": "2022-10-08T06:13:03.355356Z",
          "shell.execute_reply": "2022-10-08T06:13:03.354415Z"
        },
        "papermill": {
          "duration": 2.07347,
          "end_time": "2022-10-08T06:13:03.357459",
          "exception": false,
          "start_time": "2022-10-08T06:13:01.283989",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "3f0aac1857a846888c4f18173282a80d"
          ]
        },
        "id": "faf2c466",
        "outputId": "0130affe-c101-4e1e-f0c8-83092858c3cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f0aac1857a846888c4f18173282a80d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/88 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 0\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a645acf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:03.385485Z",
          "iopub.status.busy": "2022-10-08T06:13:03.385186Z",
          "iopub.status.idle": "2022-10-08T06:13:03.659158Z",
          "shell.execute_reply": "2022-10-08T06:13:03.658125Z"
        },
        "papermill": {
          "duration": 0.290278,
          "end_time": "2022-10-08T06:13:03.661294",
          "exception": false,
          "start_time": "2022-10-08T06:13:03.371016",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "d1ed927666e54a47a14172fcf19d4f28"
          ]
        },
        "id": "2a645acf",
        "outputId": "40b7743e-18a9-4559-cc30-d5fcd6ebb34f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1ed927666e54a47a14172fcf19d4f28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/11 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 10567\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"validation\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95da398",
      "metadata": {
        "papermill": {
          "duration": 0.013401,
          "end_time": "2022-10-08T06:13:03.688227",
          "exception": false,
          "start_time": "2022-10-08T06:13:03.674826",
          "status": "completed"
        },
        "tags": [],
        "id": "a95da398"
      },
      "source": [
        "We can see that in train we have only one answer for all the samples.But in validation data there are 10567 samples with multiple answers.\n",
        "\n",
        "Before getting to the problem Let us understand how a question answering problem is solved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3469716d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:03.716245Z",
          "iopub.status.busy": "2022-10-08T06:13:03.715949Z",
          "iopub.status.idle": "2022-10-08T06:13:03.745679Z",
          "shell.execute_reply": "2022-10-08T06:13:03.744671Z"
        },
        "papermill": {
          "duration": 0.046031,
          "end_time": "2022-10-08T06:13:03.747722",
          "exception": false,
          "start_time": "2022-10-08T06:13:03.701691",
          "status": "completed"
        },
        "tags": [],
        "id": "3469716d",
        "outputId": "63dd3e69-b7c5-4e3d-82de-40c6ed5a5ece"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Lets sample some dataset so that we can reduce training time.\n",
        "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(8000)])\n",
        "dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(2000)])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9db76b",
      "metadata": {
        "papermill": {
          "duration": 0.01335,
          "end_time": "2022-10-08T06:13:03.774483",
          "exception": false,
          "start_time": "2022-10-08T06:13:03.761133",
          "status": "completed"
        },
        "tags": [],
        "id": "1d9db76b"
      },
      "source": [
        "**Data Preprocessing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5c46a1",
      "metadata": {
        "papermill": {
          "duration": 0.013317,
          "end_time": "2022-10-08T06:13:03.801246",
          "exception": false,
          "start_time": "2022-10-08T06:13:03.787929",
          "status": "completed"
        },
        "tags": [],
        "id": "2d5c46a1"
      },
      "source": [
        "**How to label the dataset?**\n",
        "\n",
        "Bert needs to know span of tokens corresponding text containing the answer. In question answering system, corresponding to each input token we have 2 outputs.ie, the start and end position. These can be (0,0),(1,0) or (0,1).ie each label can be either 1 or 0. \n",
        "\n",
        "We will label as  (1,0) correponding to the starting of answer token among the input tokens to the model. Similarly we have (0,1) corresponding to ending of answer token. We will have (0,0) label correponding to all other tokens.Let me explain it by taking an example.\n",
        "\n",
        "```\n",
        "eg: Question ->  which is your favorite place?\n",
        "Context -> \"My favourite place is Empire state building\"\n",
        "Answer -> \"Empire state building\"\n",
        "```\n",
        "\n",
        "After tokenization it may looks as follows (if max_length = 20)\n",
        "\n",
        "```\n",
        "[CLS],[which], [is] , [your], [favourite] , [place] , [?] , [SEP], [My] [favourite], [place], [is], [Empire], [state], [building], [SEP],[PAD], [PAD], [PAD] [PAD] [PAD]\n",
        "```\n",
        "\n",
        "\n",
        "So the labels of each token will be as follows:\n",
        "\n",
        "```\n",
        "[CLS] -> [0,0]\n",
        "[which] -> [0,0]\n",
        "[is] -> [0,0]\n",
        "[your] -> [0,0]\n",
        "[favourite] -> [0,0]\n",
        "[place] -> [0,0]\n",
        "[?] -> [0,0]\n",
        "[SEP] -> [0,0]\n",
        "[My] -> [0,0]\n",
        "[favourite] -> [0,0]\n",
        "[place] -> [0,0]\n",
        "[is] -> [0,0]\n",
        "[Empire] -> [1,0]\n",
        "[state] -> [0,0]\n",
        "[building] -> [0,1]\n",
        "[SEP] -> [0,0]\n",
        "[PAD] -> [0,0]\n",
        "[PAD] -> [0,0]\n",
        "[PAD] -> [0,0]\n",
        "[PAD] -> [0,0]\n",
        "\n",
        "```\n",
        "\n",
        "Finally the labels will be :\n",
        "\n",
        "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0\n",
        "\n",
        "The model will predict the start and end logit of each token.\n",
        "eg:\n",
        "```\n",
        "[0.2, 0.4 ,0.3 , 0.5, 0.6, 0.4, 0.1, 0.3, 0.2, 0.4, 0.3, 0.5 ,0.9 , 0.6, 0.7, 0.8, 0.23, 0.31, 0.12]\n",
        "\n",
        "[0.3, 0.1 ,0.5 , 0.5, 0.3, 0.2, 0.1, 0.13, 0.22, 0.61, 0.23, 0.51 ,0.4 , 0.83, 0.45, 0.12, 0.3, 0.51, 0.22]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Note: \n",
        "\n",
        "In case of hugging face library, we do not need to provide labels.We just need to start and end position of tokens. For example in above case we have start_position = 12 and end_position = 14. Model will provide start logits and end logits as output annd we can apply arg max to find start position and end position. From above logits we can find argmax values as 12 and 14 for start and end positions respectivly. From these we can  get the answer from context as \"Empire state building\".\n",
        "On Prediction we will have probability prediction corresponding to each start and end positions. Thus we can find the start position with highest probability and end position with highest probability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fdd91c",
      "metadata": {
        "papermill": {
          "duration": 0.013286,
          "end_time": "2022-10-08T06:13:03.827996",
          "exception": false,
          "start_time": "2022-10-08T06:13:03.814710",
          "status": "completed"
        },
        "tags": [],
        "id": "58fdd91c"
      },
      "source": [
        "**How to handle long contexts??**\n",
        "\n",
        "Here we have relatively smaller context length. But what if its very large. Then during truncation there is a chance that answer might get truncated and context will miss the answer.To solve this problem we create several features of different pieces of context. The only thing we must aware is to add enough overlap between contexts.\n",
        "This can be done by tokenizer itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a84317e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:03.856768Z",
          "iopub.status.busy": "2022-10-08T06:13:03.855981Z",
          "iopub.status.idle": "2022-10-08T06:13:13.821807Z",
          "shell.execute_reply": "2022-10-08T06:13:13.820853Z"
        },
        "papermill": {
          "duration": 9.983002,
          "end_time": "2022-10-08T06:13:13.824481",
          "exception": false,
          "start_time": "2022-10-08T06:13:03.841479",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "593757f0ada94b1582abdf24ed1e9477",
            "6156d681ad144ea7a15060b8900e0f59",
            "524dd750663f4a6892c72b3d1c145928",
            "b3d4fa1bbd4f43eab213d3b83b460957"
          ]
        },
        "id": "7a84317e",
        "outputId": "98e4c850-b02b-4e4b-c9d9-a0564306f1c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "593757f0ada94b1582abdf24ed1e9477",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6156d681ad144ea7a15060b8900e0f59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "524dd750663f4a6892c72b3d1c145928",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3d4fa1bbd4f43eab213d3b83b460957",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 4 examples gave 2 features.\n",
            "Here is where each comes from: [0, 0].\n",
            "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            " \n",
            "Context :  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer:  ['Saint Bernadette Soubirous']\n",
            "--------------------------------------------------\n",
            "Context piece 1\n",
            "[SEP] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues [SEP]\n",
            " \n",
            "Context piece 2\n",
            "[SEP] sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP]\n",
            " \n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model_checkpoint = \"bert-base-cased\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "context = dataset[\"train\"][0][\"context\"]\n",
        "question = dataset[\"train\"][0][\"question\"]\n",
        "answer = dataset[\"train\"][0][\"answers\"][\"text\"]\n",
        "\n",
        "\n",
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=160,\n",
        "    truncation=\"only_second\",  # only to truncate context\n",
        "    stride=70,  # no of overlapping tokens  between concecute context pieces\n",
        "    return_overflowing_tokens=True,  #to let tokenizer know we want overflow tokens\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
        "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n",
        "\n",
        "print('Question: ',question)\n",
        "print(' ')\n",
        "print('Context : ',context)\n",
        "print(' ')\n",
        "print('Answer: ', answer)\n",
        "print('--'*25)\n",
        "\n",
        "for i,ids in enumerate(inputs[\"input_ids\"]):\n",
        "    print('Context piece', i+1)\n",
        "    print(tokenizer.decode(ids[ids.index(102):]))\n",
        "    print(' ')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8cd46b",
      "metadata": {
        "papermill": {
          "duration": 0.021338,
          "end_time": "2022-10-08T06:13:13.868059",
          "exception": false,
          "start_time": "2022-10-08T06:13:13.846721",
          "status": "completed"
        },
        "tags": [],
        "id": "ea8cd46b"
      },
      "source": [
        "We can see that our entire context is divided to 4 overlaping pieces and answer only appears in 3rd and 4th piece. Here we created some contexts without answers, For those question,context pair we have label as start_position = end_position = 0.\n",
        "We will also set the same labels in unfortunate cases where answer has been truncated either at start or end. For the examples where answer is fully in context the labels will be the index of the token where the answer starts and the index of the token where the answer ends."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "949edf9d",
      "metadata": {
        "papermill": {
          "duration": 0.022079,
          "end_time": "2022-10-08T06:13:13.911047",
          "exception": false,
          "start_time": "2022-10-08T06:13:13.888968",
          "status": "completed"
        },
        "tags": [],
        "id": "949edf9d"
      },
      "source": [
        "```\n",
        "Note: \n",
        "Basically offset_mapping -> refers to start index and end index of each token with respect to whole text.\n",
        "\n",
        "overflow_to_sample_mapping (overflow-tokens) -> indicates from which base context the sub context came from. \n",
        "eg - [0,1,1] indicates first datapoint is from 1st context. 2 and 3rd from second context. \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad5e15c",
      "metadata": {
        "papermill": {
          "duration": 0.016075,
          "end_time": "2022-10-08T06:13:13.957661",
          "exception": false,
          "start_time": "2022-10-08T06:13:13.941586",
          "status": "completed"
        },
        "tags": [],
        "id": "1ad5e15c"
      },
      "source": [
        "**How to label the dataset if we split contexts of longer length in to smaller contexts?**\n",
        "\n",
        "We already explained about dividing the context in to pieces. Now we will see how we can label the context after dividing it in to sub contexts. Here we need to label all tokens. \n",
        "\n",
        "* We will label all tokens as (0,0) which is not part of answer.\n",
        "\n",
        "\n",
        "* We will give (0, 0) for all tokens in context if the answer is not in the corresponding span of the context. Also in cases if only answers start index is there but being truncated or answers end index is there.\n",
        "\n",
        "* We will provide (1,0) for token with start index of answer and (0, 1) for token with end index of answer if both start and end index is present in same context piece.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac87d1f3",
      "metadata": {
        "papermill": {
          "duration": 0.014161,
          "end_time": "2022-10-08T06:13:13.986055",
          "exception": false,
          "start_time": "2022-10-08T06:13:13.971894",
          "status": "completed"
        },
        "tags": [],
        "id": "ac87d1f3"
      },
      "source": [
        "Actually Hugging face will take care of these. We only need to pass Start index and End index corresponding to each input. If no answer is present in context we need to pass start and end position as 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b28f5d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:14.017705Z",
          "iopub.status.busy": "2022-10-08T06:13:14.016953Z",
          "iopub.status.idle": "2022-10-08T06:13:19.579913Z",
          "shell.execute_reply": "2022-10-08T06:13:19.578856Z"
        },
        "papermill": {
          "duration": 5.581147,
          "end_time": "2022-10-08T06:13:19.582077",
          "exception": false,
          "start_time": "2022-10-08T06:13:14.000930",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "0078bfab6a6740ecbe6a216dd716fe83"
          ]
        },
        "id": "13b28f5d",
        "outputId": "857afbee-d3ab-49bc-fcb7-6af59b0b5d89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0078bfab6a6740ecbe6a216dd716fe83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(8000, 200)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "del tokenizer\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "def train_data_preprocess(examples):\n",
        "    \n",
        "    \"\"\"\n",
        "    generate start and end indexes of answer in context\n",
        "    \"\"\"\n",
        "    \n",
        "    def find_context_start_end_index(sequence_ids):\n",
        "        \"\"\"\n",
        "        returns the token index in whih context starts and ends\n",
        "        \"\"\"\n",
        "        token_idx = 0\n",
        "        while sequence_ids[token_idx] != 1:  #means its special tokens or tokens of queston\n",
        "            token_idx += 1                   # loop only break when context starts in tokens\n",
        "        context_start_idx = token_idx\n",
        "    \n",
        "        while sequence_ids[token_idx] == 1:\n",
        "            token_idx += 1\n",
        "        context_end_idx = token_idx - 1\n",
        "        return context_start_idx,context_end_idx  \n",
        "    \n",
        "    \n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    context = examples[\"context\"]\n",
        "    answers = examples[\"answers\"]\n",
        "    \n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        context,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,  #returns id of base context\n",
        "        return_offsets_mapping=True,  # returns (start_index,end_index) of each token\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    \n",
        "    for i,mapping_idx_pairs in enumerate(inputs['offset_mapping']):\n",
        "        context_idx = inputs['overflow_to_sample_mapping'][i]\n",
        "    \n",
        "        # from main context\n",
        "        answer = answers[context_idx]\n",
        "        answer_start_char_idx = answer['answer_start'][0]\n",
        "        answer_end_char_idx = answer_start_char_idx + len(answer['text'][0])\n",
        "\n",
        "    \n",
        "        # now we have to find it in sub contexts\n",
        "        tokens = inputs['input_ids'][i]\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "   \n",
        "        # finding the context start and end indexes wrt sub context tokens\n",
        "        context_start_idx,context_end_idx = find_context_start_end_index(sequence_ids)\n",
        "    \n",
        "        #if the answer is not fully inside context label it as (0,0)\n",
        "        # starting and end index of charecter of full context text\n",
        "        context_start_char_index = mapping_idx_pairs[context_start_idx][0]\n",
        "        context_end_char_index = mapping_idx_pairs[context_end_idx][1]\n",
        "    \n",
        "\n",
        "        #If the answer is not fully inside the context, label is (0, 0)\n",
        "        if (context_start_char_index > answer_start_char_idx) or (\n",
        "            context_end_char_index < answer_end_char_idx):\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "    \n",
        "        else:\n",
        "\n",
        "            # else its start and end token positions\n",
        "            # here idx indicates index of token\n",
        "            idx = context_start_idx\n",
        "            while idx <= context_end_idx and mapping_idx_pairs[idx][0] <= answer_start_char_idx:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)  \n",
        "        \n",
        "\n",
        "            idx = context_end_idx\n",
        "            while idx >= context_start_idx and mapping_idx_pairs[idx][1] > answer_end_char_idx:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "    \n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "    \n",
        "train_sample = dataset[\"train\"].select([i for i in range(200)])\n",
        "    \n",
        "train_dataset = train_sample.map(\n",
        "    train_data_preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "len(dataset[\"train\"]),len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e65f9d4",
      "metadata": {
        "papermill": {
          "duration": 0.014167,
          "end_time": "2022-10-08T06:13:19.611505",
          "exception": false,
          "start_time": "2022-10-08T06:13:19.597338",
          "status": "completed"
        },
        "tags": [],
        "id": "0e65f9d4"
      },
      "source": [
        "We can see a increase in number of datapoints after the tokenization method we used.Lets compare the values before and after tokenization.We will print some of the questions,context and answers after tokenization and compare with the original one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9642730",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:19.641761Z",
          "iopub.status.busy": "2022-10-08T06:13:19.641479Z",
          "iopub.status.idle": "2022-10-08T06:13:19.711078Z",
          "shell.execute_reply": "2022-10-08T06:13:19.710099Z"
        },
        "papermill": {
          "duration": 0.088189,
          "end_time": "2022-10-08T06:13:19.714266",
          "exception": false,
          "start_time": "2022-10-08T06:13:19.626077",
          "status": "completed"
        },
        "tags": [],
        "id": "e9642730",
        "outputId": "04512c78-2228-49da-823e-45d4298cd8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['Saint Bernadette Soubirous']\n",
            " \n",
            "Start and end index of text:  515 541\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "saint bernadette soubirous\n",
            " \n",
            "Start pos and end pos of tokens:  130 138\n",
            "________________________________________________________________________________\n",
            "1\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "What is in front of the Notre Dame Main Building?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['a copper statue of Christ']\n",
            " \n",
            "Start and end index of text:  188 213\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] what is in front of the notre dame main building? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "a copper statue of christ\n",
            " \n",
            "Start pos and end pos of tokens:  52 57\n",
            "________________________________________________________________________________\n",
            "2\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['the Main Building']\n",
            " \n",
            "Start and end index of text:  279 296\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] the basilica of the sacred heart at notre dame is beside to which structure? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "the main building\n",
            " \n",
            "Start pos and end pos of tokens:  81 84\n",
            "________________________________________________________________________________\n",
            "3\n",
            "----\n",
            "Theoretical values :\n",
            " \n",
            "Question: \n",
            "What is the Grotto at Notre Dame?\n",
            " \n",
            "Context: \n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            " \n",
            "Answer: \n",
            "['a Marian place of prayer and reflection']\n",
            " \n",
            "Start and end index of text:  381 420\n",
            "--------------------------------------------------------------------------------\n",
            "Values after tokenization:\n",
            " \n",
            "Question: \n",
            "[CLS] what is the grotto at notre dame? [SEP]\n",
            " \n",
            "Context: \n",
            "architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            " \n",
            "Answer: \n",
            "a marian place of prayer and reflection\n",
            " \n",
            "Start pos and end pos of tokens:  95 102\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def print_context_and_answer(idx,mini_ds=dataset[\"train\"]):\n",
        "    \n",
        "    print(idx)\n",
        "    print('----')\n",
        "    question = mini_ds[idx]['question']\n",
        "    context = mini_ds[idx]['context']\n",
        "    answer = mini_ds[idx]['answers']['text']\n",
        "    print('Theoretical values :')\n",
        "    print(' ')\n",
        "    print('Question: ')\n",
        "    print(question)\n",
        "    print(' ')\n",
        "    print('Context: ')\n",
        "    print(context)\n",
        "    print(' ')\n",
        "    print('Answer: ')\n",
        "    print(answer)\n",
        "    print(' ')\n",
        "    answer_start_char_idx = mini_ds[idx]['answers']['answer_start'][0]\n",
        "    answer_end_char_idx = answer_start_char_idx + len(mini_ds[idx]['answers']['text'][0])\n",
        "    print('Start and end index of text: ',answer_start_char_idx,answer_end_char_idx)\n",
        "    print('----'*20)\n",
        "    print('Values after tokenization:')\n",
        "    \n",
        "\n",
        "    #answer\n",
        "    sep_tok_index = train_dataset[idx]['input_ids'].index(102) #get index for [SEP]\n",
        "    question_ = train_dataset[idx]['input_ids'][:sep_tok_index+1]\n",
        "    question_decoded = tokenizer.decode(question_) \n",
        "    context_ = train_dataset[idx]['input_ids'][sep_tok_index+1:]\n",
        "    context_decoded = tokenizer.decode(context_) \n",
        "    start_idx = train_dataset[idx]['start_positions']\n",
        "    end_idx = train_dataset[idx]['end_positions']\n",
        "    answer_toks = train_dataset[idx]['input_ids'][start_idx:end_idx]\n",
        "    answer_decoded = tokenizer.decode(answer_toks)\n",
        "    print(' ')\n",
        "    print('Question: ')\n",
        "    print(question_decoded)\n",
        "    print(' ')\n",
        "    print('Context: ')\n",
        "    print(context_decoded)\n",
        "    print(' ')\n",
        "    print('Answer: ')\n",
        "    print(answer_decoded)\n",
        "    print(' ')\n",
        "    print('Start pos and end pos of tokens: ',train_dataset[idx]['start_positions'],train_dataset[idx]['end_positions'])\n",
        "    print('____'*20)\n",
        "    \n",
        "    \n",
        "print_context_and_answer(0)\n",
        "print_context_and_answer(1)\n",
        "print_context_and_answer(2)\n",
        "print_context_and_answer(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97895ff",
      "metadata": {
        "papermill": {
          "duration": 0.014524,
          "end_time": "2022-10-08T06:13:19.743541",
          "exception": false,
          "start_time": "2022-10-08T06:13:19.729017",
          "status": "completed"
        },
        "tags": [],
        "id": "a97895ff"
      },
      "source": [
        "\n",
        "\n",
        "<a class=\"anchor\" id=\"section4\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">Understanding Metrics needed for Evaluation</h2>\n",
        "\n",
        "**How to evaluate the model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607e9cff",
      "metadata": {
        "papermill": {
          "duration": 0.01426,
          "end_time": "2022-10-08T06:13:19.772485",
          "exception": false,
          "start_time": "2022-10-08T06:13:19.758225",
          "status": "completed"
        },
        "tags": [],
        "id": "607e9cff"
      },
      "source": [
        "Lets take a small eval set.Here we donot need to do much preprocesing. . We will use the pretrained \"distilbert-base-uncased\" weights which is not fine tuned and lets see how the model performs.\n",
        "\n",
        "* We will set offset to None for all those questions part of the data.\n",
        "* We will also append base context id to each sample\n",
        "\n",
        "\n",
        "We will evalue using our untuned bert-base model and lets see the performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df3f4caa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:19.802982Z",
          "iopub.status.busy": "2022-10-08T06:13:19.802710Z",
          "iopub.status.idle": "2022-10-08T06:13:23.451732Z",
          "shell.execute_reply": "2022-10-08T06:13:23.450459Z"
        },
        "papermill": {
          "duration": 3.666527,
          "end_time": "2022-10-08T06:13:23.453677",
          "exception": false,
          "start_time": "2022-10-08T06:13:19.787150",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "8cc7e206f56f420c9bc93e8174b1511b"
          ]
        },
        "id": "df3f4caa",
        "outputId": "3d830c9f-e70a-4c5e-8e22-eef938f6f390"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cc7e206f56f420c9bc93e8174b1511b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def preprocess_validation_examples(examples):\n",
        "    \"\"\"\n",
        "    preprocessing validation data\n",
        "    \"\"\"\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    base_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        \n",
        "        # take the base id (ie in cases of overflow happens we get base id)\n",
        "        base_context_idx = sample_map[i]\n",
        "        base_ids.append(examples[\"id\"][base_context_idx])\n",
        "        \n",
        "        # sequence id indicates the input. 0 for first input and 1 for second input\n",
        "        # and None for special tokens by default\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        # for Question tokens provide offset_mapping as None\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"base_id\"] = base_ids\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# del tokenizer\n",
        "\n",
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "data_val_sample = dataset[\"validation\"].select([i for i in range(100)])\n",
        "eval_set = data_val_sample.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"validation\"].column_names,\n",
        ")\n",
        "len(eval_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ffb50f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:23.485536Z",
          "iopub.status.busy": "2022-10-08T06:13:23.485239Z",
          "iopub.status.idle": "2022-10-08T06:13:37.382278Z",
          "shell.execute_reply": "2022-10-08T06:13:37.381378Z"
        },
        "papermill": {
          "duration": 13.91499,
          "end_time": "2022-10-08T06:13:37.384420",
          "exception": false,
          "start_time": "2022-10-08T06:13:23.469430",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "a45e9337d6ce421981ef641ea34bf003"
          ]
        },
        "id": "b3ffb50f",
        "outputId": "6939de37-45f8-4b62-9ac0-b2eec6b9d529"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a45e9337d6ce421981ef641ea34bf003",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((100, 512), (100, 512))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "\n",
        "# del tokenizer\n",
        "# take a small sample\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"base_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "checkpoint =  \"distilbert-base-uncased\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(checkpoint).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**batch)\n",
        "    \n",
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()\n",
        "\n",
        "start_logits.shape,end_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e565e9",
      "metadata": {
        "papermill": {
          "duration": 0.015246,
          "end_time": "2022-10-08T06:13:37.415629",
          "exception": false,
          "start_time": "2022-10-08T06:13:37.400383",
          "status": "completed"
        },
        "tags": [],
        "id": "33e565e9"
      },
      "source": [
        "We will evaluate our model using Evaluate library. We use 2 metrics for evaluation.\n",
        "\n",
        "1. Exact match\n",
        "2. f1 score\n",
        "\n",
        "**Exact Match**\n",
        "\n",
        "For each question-answer pair if the charecters of the models prediction exactly match with charecters of true answer then EM=1 else 0.When assessing against a negative example, if the model predicts any text at all, it automatically receives a 0 for that example\n",
        "\n",
        "**F1 score**\n",
        "\n",
        "F1 score depends up on precision and recall.\n",
        "```\n",
        "f1 score = 2 * (precision * recall)/ precision + recall\n",
        "\n",
        "```\n",
        "\n",
        "If we take the theoritical answers and predicted answers,the number of shared words between theoritical and predicted answer is the basis for f1 score.precision is the ratio of the number of shared words to the total number of words in the prediction, and recall is the ratio of the number of shared words to the total number of words in the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ded870",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:37.448114Z",
          "iopub.status.busy": "2022-10-08T06:13:37.447226Z",
          "iopub.status.idle": "2022-10-08T06:13:48.325501Z",
          "shell.execute_reply": "2022-10-08T06:13:48.324287Z"
        },
        "papermill": {
          "duration": 10.897495,
          "end_time": "2022-10-08T06:13:48.328357",
          "exception": false,
          "start_time": "2022-10-08T06:13:37.430862",
          "status": "completed"
        },
        "tags": [],
        "id": "07ded870",
        "outputId": "8e5dd357-5858-486c-bf9f-80b4c3bbc091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting evaluate\r\n",
            "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m782.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.8.1)\r\n",
            "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.5.1)\r\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\r\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\r\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.12.0)\r\n",
            "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.0.0)\r\n",
            "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.1.0)\r\n",
            "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.13)\r\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.8.2)\r\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\r\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\r\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\r\n",
            "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\r\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (5.0.0)\r\n",
            "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\r\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\r\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\r\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\r\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\r\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.6.15.2)\r\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\r\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\r\n",
            "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\r\n",
            "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\r\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\r\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\r\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\r\n",
            "Installing collected packages: evaluate\r\n",
            "Successfully installed evaluate-0.2.2\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8f82ee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:48.362596Z",
          "iopub.status.busy": "2022-10-08T06:13:48.362236Z",
          "iopub.status.idle": "2022-10-08T06:13:50.904065Z",
          "shell.execute_reply": "2022-10-08T06:13:50.903107Z"
        },
        "papermill": {
          "duration": 2.56155,
          "end_time": "2022-10-08T06:13:50.906450",
          "exception": false,
          "start_time": "2022-10-08T06:13:48.344900",
          "status": "completed"
        },
        "tags": [],
        "id": "4a8f82ee"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import evaluate\n",
        "\n",
        "def predict_answers_and_evaluate(start_logits,end_logits,eval_set,examples):\n",
        "    \"\"\"\n",
        "    make predictions \n",
        "    Args:\n",
        "    start_logits : strat_position prediction logits\n",
        "    end_logits: end_position prediction logits\n",
        "    eval_set: processed val data\n",
        "    examples: unprocessed val data with context text\n",
        "    \"\"\"\n",
        "    # appending all id's corresponding to the base context id\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(eval_set):\n",
        "        example_to_features[feature[\"base_id\"]].append(idx)\n",
        "\n",
        "    n_best = 20\n",
        "    max_answer_length = 30\n",
        "    predicted_answers = []\n",
        "\n",
        "    for example in examples:\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # looping through each sub contexts corresponding to a context and finding\n",
        "        # answers\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "        \n",
        "            # sorting the predictions of all hidden states and taking best n_best prediction\n",
        "            # means taking the index of top 20 tokens\n",
        "            start_indexes = np.argsort(start_logit).tolist()[::-1][:n_best]\n",
        "            end_indexes = np.argsort(end_logit).tolist()[::-1][:n_best]\n",
        "        \n",
        "    \n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                \n",
        "                    # Skip answers that are not fully in the context\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                       ):\n",
        "                        continue\n",
        "\n",
        "                    answers.append({\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                        })\n",
        "\n",
        "    \n",
        "            # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "    \n",
        "    metric = evaluate.load(\"squad\")\n",
        "\n",
        "    theoretical_answers = [\n",
        "            {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n",
        "    ]\n",
        "    \n",
        "    metric_ = metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
        "    return predicted_answers,metric_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ad15b7",
      "metadata": {
        "papermill": {
          "duration": 0.01567,
          "end_time": "2022-10-08T06:13:50.938557",
          "exception": false,
          "start_time": "2022-10-08T06:13:50.922887",
          "status": "completed"
        },
        "tags": [],
        "id": "01ad15b7"
      },
      "source": [
        "Let us evaluate the model.This metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dade3f0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:50.971892Z",
          "iopub.status.busy": "2022-10-08T06:13:50.971043Z",
          "iopub.status.idle": "2022-10-08T06:13:58.487006Z",
          "shell.execute_reply": "2022-10-08T06:13:58.485885Z"
        },
        "papermill": {
          "duration": 7.534831,
          "end_time": "2022-10-08T06:13:58.489114",
          "exception": false,
          "start_time": "2022-10-08T06:13:50.954283",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "c4f4850a226f490fb30ade628fe123ce",
            "ec83199e70b54c98be852de5f342e958"
          ]
        },
        "id": "2dade3f0",
        "outputId": "a9fb328b-7b92-4cdf-80d8-96775c1b1f36"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4f4850a226f490fb30ade628fe123ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec83199e70b54c98be852de5f342e958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'exact_match': 0.0, 'f1': 3.9449777275864233}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_answers,metrics_ = predict_answers_and_evaluate(start_logits,end_logits,eval_set,data_val_sample)\n",
        "metrics_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7591ab9a",
      "metadata": {
        "papermill": {
          "duration": 0.015829,
          "end_time": "2022-10-08T06:13:58.521894",
          "exception": false,
          "start_time": "2022-10-08T06:13:58.506065",
          "status": "completed"
        },
        "tags": [],
        "id": "7591ab9a"
      },
      "source": [
        "We have very poor score as expected. Now we will fine tune the model and will see the performance in whole validation dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a8a0b09",
      "metadata": {
        "papermill": {
          "duration": 0.015983,
          "end_time": "2022-10-08T06:13:58.553946",
          "exception": false,
          "start_time": "2022-10-08T06:13:58.537963",
          "status": "completed"
        },
        "tags": [],
        "id": "2a8a0b09"
      },
      "source": [
        "\n",
        "<a class=\"anchor\" id=\"section5\"></a>\n",
        "<h2 style=\"color:green;font-size: 2em;\">Training a Question Answering System based on Bert</h2>\n",
        "\n",
        "Lets again load the dataset from fresh. We will sample a small portion of dataset for training. You can train with full data if you have enough resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5d6128",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:58.588692Z",
          "iopub.status.busy": "2022-10-08T06:13:58.587236Z",
          "iopub.status.idle": "2022-10-08T06:13:59.202784Z",
          "shell.execute_reply": "2022-10-08T06:13:59.201730Z"
        },
        "papermill": {
          "duration": 0.6348,
          "end_time": "2022-10-08T06:13:59.204956",
          "exception": false,
          "start_time": "2022-10-08T06:13:58.570156",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "cd8e4a9c8059435895dc0fa7f9e672e7"
          ]
        },
        "id": "7b5d6128",
        "outputId": "716bce6c-45ee-4bb1-e0a4-ac2c1c65e1ab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd8e4a9c8059435895dc0fa7f9e672e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "#lets sample a small dataset\n",
        "dataset['train'] = dataset['train'].select([i for i in range(5000)])\n",
        "dataset['validation'] = dataset['validation'].select([i for i in range(500)])\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5f7216",
      "metadata": {
        "papermill": {
          "duration": 0.016474,
          "end_time": "2022-10-08T06:13:59.238756",
          "exception": false,
          "start_time": "2022-10-08T06:13:59.222282",
          "status": "completed"
        },
        "tags": [],
        "id": "8d5f7216"
      },
      "source": [
        "Let us define a Pyorch dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81f80b9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:59.274248Z",
          "iopub.status.busy": "2022-10-08T06:13:59.272808Z",
          "iopub.status.idle": "2022-10-08T06:13:59.281971Z",
          "shell.execute_reply": "2022-10-08T06:13:59.281125Z"
        },
        "papermill": {
          "duration": 0.028563,
          "end_time": "2022-10-08T06:13:59.283944",
          "exception": false,
          "start_time": "2022-10-08T06:13:59.255381",
          "status": "completed"
        },
        "tags": [],
        "id": "f81f80b9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class DataQA(Dataset):\n",
        "    def __init__(self, dataset,mode=\"train\"):\n",
        "        self.mode = mode\n",
        "        \n",
        "        \n",
        "        if self.mode == \"train\":\n",
        "            # sampling\n",
        "            self.dataset = dataset[\"train\"]\n",
        "            self.data = self.dataset.map(train_data_preprocess,\n",
        "                                                      batched=True,\n",
        "                            remove_columns= dataset[\"train\"].column_names)\n",
        "        \n",
        "        else:\n",
        "            self.dataset = dataset[\"validation\"]\n",
        "            self.data = self.dataset.map(preprocess_validation_examples,\n",
        "            batched=True,remove_columns = dataset[\"validation\"].column_names,\n",
        "               )\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        out = {}\n",
        "        example = self.data[idx]\n",
        "        out['input_ids'] = torch.tensor(example['input_ids'])\n",
        "        out['attention_mask'] = torch.tensor(example['attention_mask'])\n",
        "\n",
        "        \n",
        "        if self.mode == \"train\":\n",
        "\n",
        "            out['start_positions'] = torch.unsqueeze(torch.tensor(example['start_positions']),dim=0)\n",
        "            out['end_positions'] = torch.unsqueeze(torch.tensor(example['end_positions']),dim=0)\n",
        "            \n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffcaae8d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:13:59.317846Z",
          "iopub.status.busy": "2022-10-08T06:13:59.317571Z",
          "iopub.status.idle": "2022-10-08T06:15:03.861930Z",
          "shell.execute_reply": "2022-10-08T06:15:03.860823Z"
        },
        "papermill": {
          "duration": 64.564705,
          "end_time": "2022-10-08T06:15:03.865198",
          "exception": false,
          "start_time": "2022-10-08T06:13:59.300493",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "5fa34bc5337843ca86fb633837fa60f6",
            "199be0773a124a0d90a77b04eeb58bc3"
          ]
        },
        "id": "ffcaae8d",
        "outputId": "cd1b0c0f-af62-4436-e2ed-3bf603b0445a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fa34bc5337843ca86fb633837fa60f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "199be0773a124a0d90a77b04eeb58bc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  torch.Size([512])\n",
            "attention_mask :  torch.Size([512])\n",
            "start_positions :  torch.Size([1])\n",
            "end_positions :  torch.Size([1])\n",
            "--------------------------------------------------------------------------------\n",
            "____________________________________________________________________________________________________\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids :  512\n",
            "attention_mask :  512\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "\n",
        "\n",
        "train_dataset = DataQA(dataset,mode=\"train\")\n",
        "val_dataset = DataQA(dataset,mode=\"validation\")\n",
        "\n",
        "\n",
        "\n",
        "for i,d in enumerate(train_dataset):\n",
        "    for k in d.keys():\n",
        "        print(k + ' : ', d[k].shape)\n",
        "    print('--'*40)\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "        \n",
        "print('__'*50)\n",
        "\n",
        "for i,d in enumerate(val_dataset):\n",
        "    for k in d.keys():\n",
        "        print(k + ' : ', len(d[k]))\n",
        "    print('--'*40)\n",
        "    \n",
        "    if i == 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992372be",
      "metadata": {
        "papermill": {
          "duration": 0.016538,
          "end_time": "2022-10-08T06:15:03.899862",
          "exception": false,
          "start_time": "2022-10-08T06:15:03.883324",
          "status": "completed"
        },
        "tags": [],
        "id": "992372be"
      },
      "source": [
        "Let us load the data in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ffb6d3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:15:03.935127Z",
          "iopub.status.busy": "2022-10-08T06:15:03.934273Z",
          "iopub.status.idle": "2022-10-08T06:15:03.951575Z",
          "shell.execute_reply": "2022-10-08T06:15:03.950078Z"
        },
        "papermill": {
          "duration": 0.037079,
          "end_time": "2022-10-08T06:15:03.953617",
          "exception": false,
          "start_time": "2022-10-08T06:15:03.916538",
          "status": "completed"
        },
        "tags": [],
        "id": "27ffb6d3",
        "outputId": "5f8b0a42-fe4d-4ad8-b15f-66a23aa3f94e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 512])\n",
            "torch.Size([2, 512])\n",
            "torch.Size([2, 1])\n",
            "torch.Size([2, 1])\n",
            "------------------------------------------------------------\n",
            "torch.Size([2, 512])\n",
            "torch.Size([2, 512])\n"
          ]
        }
      ],
      "source": [
        "from transformers import default_data_collator\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=default_data_collator,\n",
        "    batch_size=2,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    val_dataset, collate_fn=default_data_collator, batch_size=2\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for batch in train_dataloader:\n",
        "   print(batch['input_ids'].shape)\n",
        "   print(batch['attention_mask'].shape)\n",
        "   print(batch['start_positions'].shape)\n",
        "   print(batch['end_positions'].shape)\n",
        "   break\n",
        "\n",
        "print('---'*20)\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "   print(batch['input_ids'].shape)\n",
        "   print(batch['attention_mask'].shape)\n",
        "   break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9135e3d7",
      "metadata": {
        "papermill": {
          "duration": 0.017057,
          "end_time": "2022-10-08T06:15:03.987603",
          "exception": false,
          "start_time": "2022-10-08T06:15:03.970546",
          "status": "completed"
        },
        "tags": [],
        "id": "9135e3d7"
      },
      "source": [
        "**Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0e24be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:15:04.022384Z",
          "iopub.status.busy": "2022-10-08T06:15:04.022090Z",
          "iopub.status.idle": "2022-10-08T06:15:05.729338Z",
          "shell.execute_reply": "2022-10-08T06:15:05.728380Z"
        },
        "papermill": {
          "duration": 1.72732,
          "end_time": "2022-10-08T06:15:05.731570",
          "exception": false,
          "start_time": "2022-10-08T06:15:04.004250",
          "status": "completed"
        },
        "tags": [],
        "id": "aa0e24be",
        "outputId": "aedf1b92-f06f-44d4-cd61-7ab8c88c33df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Available device: {device}')\n",
        "\n",
        "checkpoint =  \"distilbert-base-uncased\"\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(checkpoint)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cbfadc",
      "metadata": {
        "papermill": {
          "duration": 0.016778,
          "end_time": "2022-10-08T06:15:05.765784",
          "exception": false,
          "start_time": "2022-10-08T06:15:05.749006",
          "status": "completed"
        },
        "tags": [],
        "id": "c0cbfadc"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c359756",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:15:05.801651Z",
          "iopub.status.busy": "2022-10-08T06:15:05.800777Z",
          "iopub.status.idle": "2022-10-08T06:15:05.830536Z",
          "shell.execute_reply": "2022-10-08T06:15:05.829296Z"
        },
        "papermill": {
          "duration": 0.049967,
          "end_time": "2022-10-08T06:15:05.832635",
          "exception": false,
          "start_time": "2022-10-08T06:15:05.782668",
          "status": "completed"
        },
        "tags": [],
        "id": "0c359756",
        "outputId": "606c3bfa-f408-44e6-f54d-713a5dc593dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5010\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import evaluate\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "print(total_steps)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5847c11b",
      "metadata": {
        "papermill": {
          "duration": 0.016826,
          "end_time": "2022-10-08T06:15:05.866379",
          "exception": false,
          "start_time": "2022-10-08T06:15:05.849553",
          "status": "completed"
        },
        "tags": [],
        "id": "5847c11b"
      },
      "source": [
        "We need processed validation data at the time of evaluation to get offsets for each context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b6efe7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:15:05.903369Z",
          "iopub.status.busy": "2022-10-08T06:15:05.902563Z",
          "iopub.status.idle": "2022-10-08T06:15:09.238747Z",
          "shell.execute_reply": "2022-10-08T06:15:09.237805Z"
        },
        "papermill": {
          "duration": 3.356096,
          "end_time": "2022-10-08T06:15:09.240855",
          "exception": false,
          "start_time": "2022-10-08T06:15:05.884759",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "8f0d14b2693a43c985b8f85ecd223493"
          ]
        },
        "id": "49b6efe7",
        "outputId": "cea1cc12-3345-4b45-d54e-0172ba791ecc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f0d14b2693a43c985b8f85ecd223493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# we need processed validation data to get offsets at the time of evaluation\n",
        "validation_processed_dataset = dataset[\"validation\"].map(preprocess_validation_examples,\n",
        "            batched=True,remove_columns = dataset[\"validation\"].column_names,\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca01e6c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-08T06:15:09.278548Z",
          "iopub.status.busy": "2022-10-08T06:15:09.278219Z",
          "iopub.status.idle": "2022-10-08T06:26:19.385757Z",
          "shell.execute_reply": "2022-10-08T06:26:19.384745Z"
        },
        "papermill": {
          "duration": 670.153439,
          "end_time": "2022-10-08T06:26:19.412333",
          "exception": false,
          "start_time": "2022-10-08T06:15:09.258894",
          "status": "completed"
        },
        "tags": [],
        "id": "ca01e6c5",
        "outputId": "c980ce2d-58b3-4a0a-e100-84fd06d0b6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "=====Epoch 1=====\n",
            "Training....\n",
            "  Batch    40  of  2,505.    Elapsed: 0:00:03.\n",
            "  Batch    80  of  2,505.    Elapsed: 0:00:06.\n",
            "  Batch   120  of  2,505.    Elapsed: 0:00:08.\n",
            "  Batch   160  of  2,505.    Elapsed: 0:00:11.\n",
            "  Batch   200  of  2,505.    Elapsed: 0:00:14.\n",
            "  Batch   240  of  2,505.    Elapsed: 0:00:17.\n",
            "  Batch   280  of  2,505.    Elapsed: 0:00:20.\n",
            "  Batch   320  of  2,505.    Elapsed: 0:00:22.\n",
            "  Batch   360  of  2,505.    Elapsed: 0:00:25.\n",
            "  Batch   400  of  2,505.    Elapsed: 0:00:28.\n",
            "  Batch   440  of  2,505.    Elapsed: 0:00:31.\n",
            "  Batch   480  of  2,505.    Elapsed: 0:00:33.\n",
            "  Batch   520  of  2,505.    Elapsed: 0:00:36.\n",
            "  Batch   560  of  2,505.    Elapsed: 0:00:39.\n",
            "  Batch   600  of  2,505.    Elapsed: 0:00:42.\n",
            "  Batch   640  of  2,505.    Elapsed: 0:00:44.\n",
            "  Batch   680  of  2,505.    Elapsed: 0:00:47.\n",
            "  Batch   720  of  2,505.    Elapsed: 0:00:50.\n",
            "  Batch   760  of  2,505.    Elapsed: 0:00:53.\n",
            "  Batch   800  of  2,505.    Elapsed: 0:00:56.\n",
            "  Batch   840  of  2,505.    Elapsed: 0:00:58.\n",
            "  Batch   880  of  2,505.    Elapsed: 0:01:01.\n",
            "  Batch   920  of  2,505.    Elapsed: 0:01:04.\n",
            "  Batch   960  of  2,505.    Elapsed: 0:01:07.\n",
            "  Batch 1,000  of  2,505.    Elapsed: 0:01:09.\n",
            "  Batch 1,040  of  2,505.    Elapsed: 0:01:12.\n",
            "  Batch 1,080  of  2,505.    Elapsed: 0:01:15.\n",
            "  Batch 1,120  of  2,505.    Elapsed: 0:01:18.\n",
            "  Batch 1,160  of  2,505.    Elapsed: 0:01:21.\n",
            "  Batch 1,200  of  2,505.    Elapsed: 0:01:23.\n",
            "  Batch 1,240  of  2,505.    Elapsed: 0:01:26.\n",
            "  Batch 1,280  of  2,505.    Elapsed: 0:01:29.\n",
            "  Batch 1,320  of  2,505.    Elapsed: 0:01:32.\n",
            "  Batch 1,360  of  2,505.    Elapsed: 0:01:34.\n",
            "  Batch 1,400  of  2,505.    Elapsed: 0:01:37.\n",
            "  Batch 1,440  of  2,505.    Elapsed: 0:01:40.\n",
            "  Batch 1,480  of  2,505.    Elapsed: 0:01:43.\n",
            "  Batch 1,520  of  2,505.    Elapsed: 0:01:46.\n",
            "  Batch 1,560  of  2,505.    Elapsed: 0:01:48.\n",
            "  Batch 1,600  of  2,505.    Elapsed: 0:01:51.\n",
            "  Batch 1,640  of  2,505.    Elapsed: 0:01:54.\n",
            "  Batch 1,680  of  2,505.    Elapsed: 0:01:57.\n",
            "  Batch 1,720  of  2,505.    Elapsed: 0:01:59.\n",
            "  Batch 1,760  of  2,505.    Elapsed: 0:02:02.\n",
            "  Batch 1,800  of  2,505.    Elapsed: 0:02:05.\n",
            "  Batch 1,840  of  2,505.    Elapsed: 0:02:08.\n",
            "  Batch 1,880  of  2,505.    Elapsed: 0:02:11.\n",
            "  Batch 1,920  of  2,505.    Elapsed: 0:02:13.\n",
            "  Batch 1,960  of  2,505.    Elapsed: 0:02:16.\n",
            "  Batch 2,000  of  2,505.    Elapsed: 0:02:19.\n",
            "  Batch 2,040  of  2,505.    Elapsed: 0:02:22.\n",
            "  Batch 2,080  of  2,505.    Elapsed: 0:02:24.\n",
            "  Batch 2,120  of  2,505.    Elapsed: 0:02:27.\n",
            "  Batch 2,160  of  2,505.    Elapsed: 0:02:30.\n",
            "  Batch 2,200  of  2,505.    Elapsed: 0:02:33.\n",
            "  Batch 2,240  of  2,505.    Elapsed: 0:02:36.\n",
            "  Batch 2,280  of  2,505.    Elapsed: 0:02:38.\n",
            "  Batch 2,320  of  2,505.    Elapsed: 0:02:41.\n",
            "  Batch 2,360  of  2,505.    Elapsed: 0:02:44.\n",
            "  Batch 2,400  of  2,505.    Elapsed: 0:02:47.\n",
            "  Batch 2,440  of  2,505.    Elapsed: 0:02:49.\n",
            "  Batch 2,480  of  2,505.    Elapsed: 0:02:52.\n",
            "\n",
            "  Average training loss: 2.52\n",
            "  Training epoch took: 0:02:54\n",
            "\n",
            "Running Validation...\n",
            "Exact match: 23.8, F1 score: 60.47823000838173\n",
            "\n",
            "  Validation took: 0:02:39\n",
            " \n",
            "=====Epoch 2=====\n",
            "Training....\n",
            "  Batch    40  of  2,505.    Elapsed: 0:00:03.\n",
            "  Batch    80  of  2,505.    Elapsed: 0:00:06.\n",
            "  Batch   120  of  2,505.    Elapsed: 0:00:08.\n",
            "  Batch   160  of  2,505.    Elapsed: 0:00:11.\n",
            "  Batch   200  of  2,505.    Elapsed: 0:00:14.\n",
            "  Batch   240  of  2,505.    Elapsed: 0:00:17.\n",
            "  Batch   280  of  2,505.    Elapsed: 0:00:19.\n",
            "  Batch   320  of  2,505.    Elapsed: 0:00:22.\n",
            "  Batch   360  of  2,505.    Elapsed: 0:00:25.\n",
            "  Batch   400  of  2,505.    Elapsed: 0:00:28.\n",
            "  Batch   440  of  2,505.    Elapsed: 0:00:31.\n",
            "  Batch   480  of  2,505.    Elapsed: 0:00:33.\n",
            "  Batch   520  of  2,505.    Elapsed: 0:00:36.\n",
            "  Batch   560  of  2,505.    Elapsed: 0:00:39.\n",
            "  Batch   600  of  2,505.    Elapsed: 0:00:42.\n",
            "  Batch   640  of  2,505.    Elapsed: 0:00:44.\n",
            "  Batch   680  of  2,505.    Elapsed: 0:00:47.\n",
            "  Batch   720  of  2,505.    Elapsed: 0:00:50.\n",
            "  Batch   760  of  2,505.    Elapsed: 0:00:53.\n",
            "  Batch   800  of  2,505.    Elapsed: 0:00:56.\n",
            "  Batch   840  of  2,505.    Elapsed: 0:00:58.\n",
            "  Batch   880  of  2,505.    Elapsed: 0:01:01.\n",
            "  Batch   920  of  2,505.    Elapsed: 0:01:04.\n",
            "  Batch   960  of  2,505.    Elapsed: 0:01:07.\n",
            "  Batch 1,000  of  2,505.    Elapsed: 0:01:09.\n",
            "  Batch 1,040  of  2,505.    Elapsed: 0:01:12.\n",
            "  Batch 1,080  of  2,505.    Elapsed: 0:01:15.\n",
            "  Batch 1,120  of  2,505.    Elapsed: 0:01:18.\n",
            "  Batch 1,160  of  2,505.    Elapsed: 0:01:21.\n",
            "  Batch 1,200  of  2,505.    Elapsed: 0:01:23.\n",
            "  Batch 1,240  of  2,505.    Elapsed: 0:01:26.\n",
            "  Batch 1,280  of  2,505.    Elapsed: 0:01:29.\n",
            "  Batch 1,320  of  2,505.    Elapsed: 0:01:32.\n",
            "  Batch 1,360  of  2,505.    Elapsed: 0:01:35.\n",
            "  Batch 1,400  of  2,505.    Elapsed: 0:01:37.\n",
            "  Batch 1,440  of  2,505.    Elapsed: 0:01:40.\n",
            "  Batch 1,480  of  2,505.    Elapsed: 0:01:43.\n",
            "  Batch 1,520  of  2,505.    Elapsed: 0:01:46.\n",
            "  Batch 1,560  of  2,505.    Elapsed: 0:01:49.\n",
            "  Batch 1,600  of  2,505.    Elapsed: 0:01:51.\n",
            "  Batch 1,640  of  2,505.    Elapsed: 0:01:54.\n",
            "  Batch 1,680  of  2,505.    Elapsed: 0:01:57.\n",
            "  Batch 1,720  of  2,505.    Elapsed: 0:02:00.\n",
            "  Batch 1,760  of  2,505.    Elapsed: 0:02:03.\n",
            "  Batch 1,800  of  2,505.    Elapsed: 0:02:05.\n",
            "  Batch 1,840  of  2,505.    Elapsed: 0:02:08.\n",
            "  Batch 1,880  of  2,505.    Elapsed: 0:02:11.\n",
            "  Batch 1,920  of  2,505.    Elapsed: 0:02:14.\n",
            "  Batch 1,960  of  2,505.    Elapsed: 0:02:17.\n",
            "  Batch 2,000  of  2,505.    Elapsed: 0:02:19.\n",
            "  Batch 2,040  of  2,505.    Elapsed: 0:02:22.\n",
            "  Batch 2,080  of  2,505.    Elapsed: 0:02:25.\n",
            "  Batch 2,120  of  2,505.    Elapsed: 0:02:28.\n",
            "  Batch 2,160  of  2,505.    Elapsed: 0:02:30.\n",
            "  Batch 2,200  of  2,505.    Elapsed: 0:02:33.\n",
            "  Batch 2,240  of  2,505.    Elapsed: 0:02:36.\n",
            "  Batch 2,280  of  2,505.    Elapsed: 0:02:39.\n",
            "  Batch 2,320  of  2,505.    Elapsed: 0:02:42.\n",
            "  Batch 2,360  of  2,505.    Elapsed: 0:02:44.\n",
            "  Batch 2,400  of  2,505.    Elapsed: 0:02:47.\n",
            "  Batch 2,440  of  2,505.    Elapsed: 0:02:50.\n",
            "  Batch 2,480  of  2,505.    Elapsed: 0:02:53.\n",
            "\n",
            "  Average training loss: 1.11\n",
            "  Training epoch took: 0:02:55\n",
            "\n",
            "Running Validation...\n",
            "Exact match: 24.8, F1 score: 61.57222757290872\n",
            "\n",
            "  Validation took: 0:02:42\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:11:10 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "import random,time\n",
        "import numpy as np\n",
        "\n",
        "# to reproduce results\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "#storing all training and validation stats\n",
        "stats = []\n",
        "\n",
        "\n",
        "#to measure total training time\n",
        "total_train_time_start = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(' ')\n",
        "    print(f'=====Epoch {epoch + 1}=====')\n",
        "    print('Training....')\n",
        "     \n",
        "    # ===============================\n",
        "    #    Train\n",
        "    # ===============================   \n",
        "    # measure how long training epoch takes\n",
        "    t0 = time.time()\n",
        "     \n",
        "    training_loss = 0\n",
        "    # loop through train data\n",
        "    model.train()\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "         \n",
        "        # we will print train time in every 40 epochs\n",
        "        if step%40 == 0 and not step == 0:\n",
        "              elapsed_time = format_time(time.time() - t0)\n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed_time))\n",
        "\n",
        "         \n",
        "       \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "            \n",
        "\n",
        "\n",
        "        #set gradients to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "        result = model(input_ids = input_ids, \n",
        "                        attention_mask = attention_mask,\n",
        "                        start_positions = start_positions,\n",
        "                        end_positions = end_positions,\n",
        "                        return_dict=True)\n",
        "         \n",
        "        loss = result.loss\n",
        "    \n",
        "        #accumulate the loss over batches so that we can calculate avg loss at the end\n",
        "        training_loss += loss.item()      \n",
        "\n",
        "        #perform backward prorpogation\n",
        "        loss.backward()\n",
        "\n",
        "        # update the gradients\n",
        "        optimizer.step()\n",
        "\n",
        "    # calculate avg loss\n",
        "    avg_train_loss = training_loss/len(train_dataloader) \n",
        " \n",
        "    # calculates training time\n",
        "    training_time = format_time(time.time() - t0)\n",
        "     \n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "    \n",
        "    \n",
        "    # ===============================\n",
        "    #    Validation\n",
        "    # ===============================\n",
        "     \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "     \n",
        "\n",
        "    start_logits,end_logits = [],[]\n",
        "    for step,batch in enumerate(eval_dataloader):\n",
        "         \n",
        "       \n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "         \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():  \n",
        "             result = model(input_ids = input_ids, \n",
        "                        attention_mask = attention_mask,return_dict=True)\n",
        "        \n",
        "\n",
        "\n",
        "        start_logits.append(result.start_logits.cpu().numpy())\n",
        "        end_logits.append(result.end_logits.cpu().numpy())\n",
        "   \n",
        "\n",
        "    start_logits = np.concatenate(start_logits)\n",
        "    end_logits = np.concatenate(end_logits)\n",
        "    # start_logits = start_logits[: len(val_dataset)]\n",
        "    # end_logits = end_logits[: len(val_dataset)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # calculating metrics\n",
        "    answers,metrics_ = predict_answers_and_evaluate(start_logits,end_logits,validation_processed_dataset,dataset[\"validation\"])\n",
        "    print(f'Exact match: {metrics_[\"exact_match\"]}, F1 score: {metrics_[\"f1\"]}')\n",
        "\n",
        "\n",
        "    print('')\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_train_time_start)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd978e6d",
      "metadata": {
        "papermill": {
          "duration": 0.027688,
          "end_time": "2022-10-08T06:26:19.464629",
          "exception": false,
          "start_time": "2022-10-08T06:26:19.436941",
          "status": "completed"
        },
        "tags": [],
        "id": "cd978e6d"
      },
      "source": [
        "**Note: You can train for more epochs with full data which will provide us a better result**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 842.672865,
      "end_time": "2022-10-08T06:26:22.650812",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-10-08T06:12:19.977947",
      "version": "2.3.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ed9d7a09e344f7c885291fa735718c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_065c29204bc7485394160692561ec1fc",
              "IPY_MODEL_1c45b7c90a144f28b0a32998a5279f8e",
              "IPY_MODEL_56b676da20c24ec78f4860cac60a4c3f"
            ],
            "layout": "IPY_MODEL_3aedd62bc93242658f99590834dc48df"
          }
        },
        "065c29204bc7485394160692561ec1fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6858a4ee3f64071aa506bc8c2ef0a71",
            "placeholder": "​",
            "style": "IPY_MODEL_0a4c9647a8ef4b90bdcc34cd7595ba18",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "1c45b7c90a144f28b0a32998a5279f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b618e5eba284d568afa4f7290f97b8a",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_788bc22ebad149c899718836a60cdf72",
            "value": 231508
          }
        },
        "56b676da20c24ec78f4860cac60a4c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b73557f8563429889ea87cbbe3a58dd",
            "placeholder": "​",
            "style": "IPY_MODEL_621ae6f6daed4fa691409524d78cad99",
            "value": " 232k/232k [00:00&lt;00:00, 1.43MB/s]"
          }
        },
        "3aedd62bc93242658f99590834dc48df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6858a4ee3f64071aa506bc8c2ef0a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4c9647a8ef4b90bdcc34cd7595ba18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b618e5eba284d568afa4f7290f97b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788bc22ebad149c899718836a60cdf72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b73557f8563429889ea87cbbe3a58dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621ae6f6daed4fa691409524d78cad99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0985202a2304e36b01a3eb5be557151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d37df4a29ce64a08ad027e4b92015fd0",
              "IPY_MODEL_c05dfa5df68849f4be9d020955a82537",
              "IPY_MODEL_a5f3aba34aee46888ce139b458438827"
            ],
            "layout": "IPY_MODEL_71429803b40e42c79c6f6a3b801321f8"
          }
        },
        "d37df4a29ce64a08ad027e4b92015fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aec1cfab5dc44211b3baf15d257c9cc1",
            "placeholder": "​",
            "style": "IPY_MODEL_2c35dd6724b74cd285ce7d312f5c0d20",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "c05dfa5df68849f4be9d020955a82537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ef02c3cacd7410bafdd94897cfa0837",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8987e161b7ba445ca7eec571d7630007",
            "value": 112
          }
        },
        "a5f3aba34aee46888ce139b458438827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c9b2cc4c2747e1b443de3ab99a9bd4",
            "placeholder": "​",
            "style": "IPY_MODEL_506b9c508a6f44f9832bfd66f7ef34fa",
            "value": " 112/112 [00:00&lt;00:00, 5.82kB/s]"
          }
        },
        "71429803b40e42c79c6f6a3b801321f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aec1cfab5dc44211b3baf15d257c9cc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c35dd6724b74cd285ce7d312f5c0d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef02c3cacd7410bafdd94897cfa0837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8987e161b7ba445ca7eec571d7630007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45c9b2cc4c2747e1b443de3ab99a9bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506b9c508a6f44f9832bfd66f7ef34fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ae938b92d274f658f9f64659402839d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d87a0ed68252402d992f3156e4dd2a96",
              "IPY_MODEL_4fc50db278024a6b8577d39f07631718",
              "IPY_MODEL_571b746f58c542e7bb1bcdfe18fd9787"
            ],
            "layout": "IPY_MODEL_c3fb4aac484645e4be878cfbf1d7f00f"
          }
        },
        "d87a0ed68252402d992f3156e4dd2a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_716c4a00b1c24dbaa47cefec6b7432b2",
            "placeholder": "​",
            "style": "IPY_MODEL_235f3f1d7e2e44d2bcfdc4c385e857da",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "4fc50db278024a6b8577d39f07631718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c72c18c7b6345b8b448482418bfe1eb",
            "max": 321,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d746f2a4c4aa4cb28e436e69004d8933",
            "value": 321
          }
        },
        "571b746f58c542e7bb1bcdfe18fd9787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e142a48568d405ebca7daaec4d061a3",
            "placeholder": "​",
            "style": "IPY_MODEL_78d42edd8719425096cb5b814f65cf0a",
            "value": " 321/321 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "c3fb4aac484645e4be878cfbf1d7f00f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "716c4a00b1c24dbaa47cefec6b7432b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "235f3f1d7e2e44d2bcfdc4c385e857da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c72c18c7b6345b8b448482418bfe1eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d746f2a4c4aa4cb28e436e69004d8933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e142a48568d405ebca7daaec4d061a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d42edd8719425096cb5b814f65cf0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39464cbb371f44ae8991adc23ed2f847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_784929e9e4fe43cdbe37b54bce84bf23",
              "IPY_MODEL_f8a016c165004ed7888709bfe66b45de",
              "IPY_MODEL_c0afb74e5b514a03bae9fd83109f9e75"
            ],
            "layout": "IPY_MODEL_76e2a243cebb40cbadccbdce944d492f"
          }
        },
        "784929e9e4fe43cdbe37b54bce84bf23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdcc5e74b6bc4ce893fee4cf872f96c1",
            "placeholder": "​",
            "style": "IPY_MODEL_4779e6adfb214f218a97c4257d3e4eb4",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "f8a016c165004ed7888709bfe66b45de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07ad6a25576f4a1bb41a0bfb12b3c8f9",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bd7b24824ff483dbdb1e9096b7b949c",
            "value": 614
          }
        },
        "c0afb74e5b514a03bae9fd83109f9e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f30b27f883dd4d0a908993d317410ee0",
            "placeholder": "​",
            "style": "IPY_MODEL_5c17c34a2c0c45b0bb50ab15f7116121",
            "value": " 614/614 [00:00&lt;00:00, 27.2kB/s]"
          }
        },
        "76e2a243cebb40cbadccbdce944d492f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdcc5e74b6bc4ce893fee4cf872f96c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4779e6adfb214f218a97c4257d3e4eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07ad6a25576f4a1bb41a0bfb12b3c8f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd7b24824ff483dbdb1e9096b7b949c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f30b27f883dd4d0a908993d317410ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c17c34a2c0c45b0bb50ab15f7116121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88dfa753c47347d9881050beaa8095e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dfe37c2201642ce8dafa65a70ae543b",
              "IPY_MODEL_f5bec9b1ace1458b995cc34e73ac92a5",
              "IPY_MODEL_d7b2805f0b2642de8e7fbb5f1bb84737"
            ],
            "layout": "IPY_MODEL_516774e959eb4ba9b6306886d8c3c078"
          }
        },
        "7dfe37c2201642ce8dafa65a70ae543b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1bfc5e73f8c4707a5f2ea05aadd6093",
            "placeholder": "​",
            "style": "IPY_MODEL_ea4f818cfb2d46eb9e19edc75483f1b4",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "f5bec9b1ace1458b995cc34e73ac92a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75a5b3c0aab74568ad0152c5229a9764",
            "max": 435658487,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc076d31ed534296a2f2389fce261fc1",
            "value": 435658487
          }
        },
        "d7b2805f0b2642de8e7fbb5f1bb84737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_543be14283d847288d2929e16bf08353",
            "placeholder": "​",
            "style": "IPY_MODEL_f996e84eb3a54ef9b229fc4aa58fa8bf",
            "value": " 436M/436M [00:10&lt;00:00, 40.0MB/s]"
          }
        },
        "516774e959eb4ba9b6306886d8c3c078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1bfc5e73f8c4707a5f2ea05aadd6093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea4f818cfb2d46eb9e19edc75483f1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75a5b3c0aab74568ad0152c5229a9764": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc076d31ed534296a2f2389fce261fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "543be14283d847288d2929e16bf08353": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f996e84eb3a54ef9b229fc4aa58fa8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}